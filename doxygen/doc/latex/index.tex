$\vert$ Read the \href{http://robotology-playground.github.io/event-driven/doxygen/doc/html/index.html}{\tt Documentation} $\vert$ Download the \href{https://github.com/robotology-playground/event-driven}{\tt Code} $\vert$

\href{https://youtu.be/xS-7xYRYSLc}{\tt }  Click the thumbnail to watch the \href{https://youtu.be/xS-7xYRYSLc}{\tt video}!

\section*{The event-\/driven Y\+A\+RP Project}

Libraries that handle neuromorphic sensors, such as the dynamic vision sensor, installed on the i\+Cub can be found here, along with algorithms to process the event-\/based data. Examples include, optical flow, corner detection and ball detection. Demo applications for controlling the robot based on event input are also found here.

\subsection*{Libraries}

\subsection*{Modules}


\begin{DoxyItemize}
\item {\bfseries Optical Flow} -- an estimate of object velocity in the visual plane is given by the rate at which the spatial location of events change over time. Such a signal manifests as a manifold in the spatio-\/temporal event space. Local velocity can be extracted by fitting planes to these manifolds. The {\ttfamily ev\+::v\+Flow} module converts the ED camera output {\ttfamily \hyperlink{classev_1_1AddressEvent}{ev\+::\+Address\+Event}} to {\ttfamily \hyperlink{classev_1_1FlowEvent}{ev\+::\+Flow\+Event}}.
\item {\bfseries Cluster Tracking} -- The movement of an object across the visual field of an ED camera produces a detailed, unbroken trace of events. Local clusters of events can be tracked by updating a tracker position as new events are observed that belong to the same trace. The spatial distribution of the events can be estimated with a Gaussian distribution. The cluster centre and distribution statistics is output from the {\ttfamily ev\+::v\+Cluster} module as a {\ttfamily \hyperlink{classev_1_1GaussianAE}{ev\+::\+Gaussian\+AE}} event.
\item {\bfseries Corner Detection} -- using an event-\/driven Harris algorithm, the full event stream is filtered to contain only the events falling on the corners of objects or structure in the scene. Corner events are useful to avoid the aperture problem and to reduce the data stream to informative events for further processing. Compared to a traditional camera, the ED corner algorithm requires less processing. {\ttfamily ev\+::v\+Corner} converts {\ttfamily \hyperlink{classev_1_1AddressEvent}{ev\+::\+Address\+Event}} to {\ttfamily \hyperlink{classev_1_1LabelledAE}{ev\+::\+Labelled\+AE}}.
\item {\bfseries Circle Detection} -- detection of circular shapes in the event stream can be performed using an ED Hough transform. As the camera moves on a robot, many background events clutter the detection algorithm. The {\ttfamily ev\+::v\+Circle} module reduces the false positive detections by using optical flow information to provide a more accurate understanding of only the most up-\/to-\/date spatial structure. {\ttfamily ev\+::v\+Circle} accepts {\ttfamily \hyperlink{classev_1_1AddressEvent}{ev\+::\+Address\+Event}} and {\ttfamily \hyperlink{classev_1_1FlowEvent}{ev\+::\+Flow\+Event}} and outputs {\ttfamily \hyperlink{classev_1_1GaussianAE}{ev\+::\+Gaussian\+AE}}.
\item {\bfseries Particle filtering} -- probabilistic filtering is used to provide a robust tracking over time. The particle filter is robust to variations in speed of the target by also sampling within the temporal dimension. A observation likelihood function that responds to a circular shape was developed to instigate a comparison with the Hough transform. The tracking position is output as {\ttfamily \hyperlink{classev_1_1GaussianAE}{ev\+::\+Gaussian\+AE}}. Future work involves adapting the filter to respond to different target shapes, and templates learned from data.
\end{DoxyItemize}

\subsection*{Applications for the i\+Cub Humanoid Robot}

\subsection*{How to Install\+:}


\begin{DoxyEnumerate}
\item Install \href{https://github.com/robotology/yarp}{\tt Y\+A\+RP} and \href{https://github.com/robotology/icub-contrib-common}{\tt icub-\/contrib-\/common} following these \href{http://wiki.icub.org/wiki/Linux:Installation_from_sources}{\tt instructions}.
\item Clone event-\/driven and install as an icub-\/contrib module.
\end{DoxyEnumerate}

\subsection*{References}

Glover, A., and Bartolozzi C. (2016) {\itshape Event-\/driven ball detection and gaze fixation in clutter}. In I\+E\+E\+E/\+R\+SJ International Conference on Intelligent Robots and Systems (I\+R\+OS), October 2016, Daejeon, Korea. {\bfseries Finalist for Robo\+Cup Best Paper Award}

Vasco V., Glover A., and Bartolozzi C. (2016) {\itshape Fast event-\/based harris corner detection exploiting the advantages of event-\/driven cameras}. In I\+E\+E\+E/\+R\+SJ International Conference on Intelligent Robots and Systems (I\+R\+OS), October 2016, Daejeon, Korea.

V. Vasco, A. Glover, Y. Tirupachuri, F. Solari, M. Chessa, and Bartolozzi C. {\itshape Vergence control with a neuromorphic i\+Cub. In I\+E\+E\+E-\/\+R\+AS International Conference on Humanoid Robots (Humanoids)}, November 2016, Mexico. 